{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082d83b0-98e4-448d-a4ec-051d3739e016",
   "metadata": {},
   "source": [
    "### [ENCODE](https://www.encodeproject.org/)数据收集和预处理\n",
    "\n",
    "- 表观数据  \n",
    "  ATAC, H3K27ac, H3K27me3, H3K36me3, H3K4me1, H3K4me3, H3K9me3  \n",
    "- 细胞系\n",
    "  选择ENCODE数据库中有以上表观数据的细胞系，筛选后包括：  \n",
    "  K562, HepG2, GM12878, MCF-7, SK-N-SH, HCT116, IMR-90  \n",
    "- 转录因子\n",
    "  以上细胞系涵盖的转录因子并且在至少4个细胞系中被测到，包含两类，`1)`数据库如[Jaspar](https://jaspar.elixir.no/)等中有相关基序记录，`2)`数据库中没有基序记录。前者可直接结合DNA，后者可能通过co-factor结合DNA。其中：\n",
    "\n",
    "  I类（33个）包括：'ATF3', 'BHLHE40', 'CEBPB', 'CTCF', 'EGR1', 'ELF1', 'ELK1', 'FOXM1', 'GABPA', 'JUND', 'MAFK', 'MYC', 'NRF1', 'REST', 'RFX5', 'SP1', 'SRF', 'TCF7L2', 'USF1', 'USF2', 'YY1', 'ZBTB33', 'ZBTB40', 'ZNF24', 'ZNF274', 'CREB1', 'FOS', 'FOXK2', 'MAX', 'MAZ', 'MXI1', 'TCF12', 'TEAD4',\n",
    "\n",
    "  II类（11个）包括：'CHD1', 'CHD2', 'EP300', 'HDAC2', 'POLR2A', 'RAD21', 'RCOR1', 'SIN3A', 'SMC3', 'TAF1', 'TARDBP',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9445ca8-d29c-4b6c-97ba-6cdf91022ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70800679-8ec2-4929-8271-b9df2b13c12f",
   "metadata": {},
   "source": [
    "#### ENCODE-Experiment report记录表及文件下载\n",
    "\n",
    "时间：2023-05-08  \n",
    "物种： 人类  \n",
    "文件：  \n",
    "1. 原始记录，从ENCODE直接下载，experiment_report_2023_5_8_3h_3m.tsv  \n",
    "2. 人类（Homo sapiens）数据，experiment_report_human_keep.tsv  \n",
    "3. 结果文件，带星标，released_files_of_assays_default_samples.csv\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d676437-f2ab-44f0-80a8-f0ecb1dd8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_file='experiment_report_2023_5_8_3h_3m.tsv'\n",
    "info=pd.read_table(info_file,skiprows=1)\n",
    "\n",
    "#选择数据\n",
    "assay_keep=['ATAC-seq', 'ChIP-seq']\n",
    "target_keep=['H3K4me1', 'H3K4me3', 'H3K9me3', 'H3K27me3', 'H3K36me3', 'H3K27ac','nan']\n",
    "\n",
    "info_human=info[info['Organism']=='Homo sapiens']\n",
    "info_human_fill=info_human.fillna('nan')\n",
    "info_human_keep=info_human_fill[info_human_fill['Assay name'].isin(assay_keep)]\n",
    "info_human_keep=info_human_keep[info_human_keep['Target of assay'].isin(target_keep)]\n",
    "\n",
    "info_file_human_keep='experiment_report_human_keep.tsv'\n",
    "info_human_keep.to_csv(info_file_human_keep,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670a9cd-e47c-4e74-8124-c67fdccf9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数，获取实验详细信息，包括结果文件等\n",
    "#\n",
    "url_org='https://www.encodeproject.org'\n",
    "\n",
    "def searcher(experiment_id,experiment_assay,target='uknown',biosample='uknown'):\n",
    "    #根据实验ID搜索实验相关数据文件，区别peak文件（bed格式）和rna水平文件（tsv格式）\n",
    "    if experiment_assay in ['ATAC-seq','DNase-seq','ChIP-seq']:\n",
    "        file_format='bed'\n",
    "        output_type='peaks&output_type=pseudoreplicated+peaks&output_type=replicated+peaks'\n",
    "    elif experiment_assay=='RNA-seq':\n",
    "        file_format='tsv'\n",
    "        output_type='gene+quantifications'\n",
    "    else:\n",
    "        print(\"experiment type wrong, one of [ATAC_seq, DNase-seq, ChIP-seq, RNA-seq]\")\n",
    "        print(\"##### accession id wrong\", experiment_id)\n",
    "    #定制搜索url（REST API）\n",
    "    url=url_org+'/search/?type=File&dataset=/experiments/%s/&file_format=%s&output_type=%s&format=json&frame=object' %(\n",
    "        experiment_id,file_format,output_type\n",
    "    )\n",
    "    headers = {'accept': 'application/json'}  #强制返回json数据\n",
    "    r=requests.get(url, headers=headers).json()\n",
    "\n",
    "    #从返回的json数据中提取文件属性\n",
    "    graphs=[]\n",
    "    for item in r['@graph']:\n",
    "        prop={}\n",
    "        prop['experiment']=experiment_id\n",
    "        prop['assay']=experiment_assay\n",
    "        prop['target_of_assay']=target\n",
    "        prop['biosample']=biosample\n",
    "        prop['accession']=item['accession']\n",
    "        prop['file_type']=item['file_type']\n",
    "        prop['file_size']=item['file_size']\n",
    "        prop['href']=item['href']\n",
    "        prop['md5sum']=item['md5sum']\n",
    "        prop['status']=item['status']\n",
    "        prop['assembly']=item['assembly']\n",
    "        prop['date_created']=item['date_created']\n",
    "        try:\n",
    "            prop['preferred_default']=item['preferred_default']  #ENCODE中带星标的文件\n",
    "        except:\n",
    "            prop['preferred_default']='#'\n",
    "        graphs.append(prop)\n",
    "    return graphs\n",
    "\n",
    "def writerx(content_list,to_file):\n",
    "    with open(to_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = [\n",
    "            'experiment',\n",
    "            'assay',\n",
    "            'target_of_assay',\n",
    "            'biosample',\n",
    "            'accession',\n",
    "            'file_type',\n",
    "            'file_size',\n",
    "            'href',\n",
    "            'md5sum',\n",
    "            'status',\n",
    "            'assembly',\n",
    "            'date_created',\n",
    "            'preferred_default']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a7e3c-0730-4e5b-a3d9-07d0dafaf2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "experiments_file='experiment_report_human_keep.tsv'\n",
    "assays_file='released_files_of_assays.csv'\n",
    "log_file='released_files_of_assays.log'\n",
    "\n",
    "experiments=[]\n",
    "with open(experiments_file,'r') as f:\n",
    "    next(f)  #skip first line\n",
    "    for line in f:\n",
    "        line=line.strip().split('\\t')\n",
    "        #Accession, Assay name, Target of assay, Biosample term name, Biosample classification\n",
    "        experiments.append((line[1],line[2],line[6],line[9]))\n",
    "print(experiments[0])\n",
    "\n",
    "files=[]\n",
    "log=open(log_file,'a')  #将日志输出到文件\n",
    "for exp in experiments:\n",
    "    print(\"##### searching...\", exp[0],exp[1],exp[2],exp[3],file=log)\n",
    "    try:\n",
    "        graph=searcher(exp[0],exp[1],exp[2],exp[3])\n",
    "        files.extend(graph)\n",
    "    except:\n",
    "        print(\"##### something wrong!\", exp[0],exp[1],exp[2],exp[3],file=log)\n",
    "        pass\n",
    "log.close()\n",
    "writerx(files,assays_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ca83d-d0e7-4b35-a7ce-a2010e8e231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #第一次搜索文件出错的assays，重新搜索，再合并文件直到完整\n",
    "# #查缺\n",
    "# experiments_add=[]\n",
    "# with open(log_file,'r') as f:\n",
    "#     for line in f:\n",
    "#         if 'wrong' in line:\n",
    "#             line=line.split()\n",
    "#             experiments_add.append(line[3])\n",
    "\n",
    "# experiments_add=info_human_keep[info_human_keep['Accession'].isin(experiments_add)]\n",
    "# experiments_add=experiments_add.iloc[:,[1,2,6,9,3]]\n",
    "\n",
    "# assays_file_add='released_files_of_assays_add.csv'\n",
    "# #搜索\n",
    "# files_add=[]\n",
    "# for i in range(experiments_add.shape[0]):\n",
    "#     print(\"##### searching...\", experiments_add.iloc[i][0],experiments_add.iloc[i][1],experiments_add.iloc[i][2],experiments_add.iloc[i][3])\n",
    "#     graph=searcher(experiments_add.iloc[i][0],experiments_add.iloc[i][1],experiments_add.iloc[i][2],experiments_add.iloc[i][3])\n",
    "#     files_add.extend(graph)\n",
    "# writerx(files_add,assays_file_add)\n",
    "# #合并\n",
    "# assays_1=pd.read_csv(assays_file)\n",
    "# assays_2=pd.read_csv(assays_file_add)\n",
    "# assays_merge=pd.concat([assays_1,assays_2])\n",
    "# print(assays_merge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4ea7a-5343-4047-af8d-772d747eaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤文件\n",
    "def get_newest(df,by_='year_created'):\n",
    "    df = df.sort_values(by = by_,ascending=True)\n",
    "    return df.iloc[-1,:]\n",
    "\n",
    "assays_fetch=assays_merge[assays_merge['assembly']=='GRCh38']  #refrence genome \n",
    "assays_fetch =assays_fetch[assays_fetch['status']=='released']  #released file\n",
    "assays_fetch=assays_fetch[assays_fetch['preferred_default']=='True']  #星标的文件\n",
    "assays_fetch['year_created']=pd.to_datetime(assays_fetch['date_created']).apply(lambda x:int(x.year))\n",
    "\n",
    "#对于一个实验含有多个带星标文件的情况，选择最新的文件（ENCODE4）\n",
    "assays_fetch_newest=assays_fetch.groupby(['experiment','assay','biosample'],as_index=False).apply(get_newest)\n",
    "assays_fetch_newest=assays_fetch_newest.sort_values(by=['biosample','assay'])\n",
    "\n",
    "print(assays_fetch_newest.shape)\n",
    "assays_fetch_file='released_files_of_assays_default_samples.csv'\n",
    "assays_fetch_newest.to_csv(assays_fetch_file,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19571589-d432-4de5-b1ad-8c7971a1b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#下载文件\n",
    "#\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a3df4b-6466-4f96-81f3-06e1056a31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def downloader(url):\n",
    "    filename=url.split('/')[-1]\n",
    "    to_file=os.path.join(to_path,filename)\n",
    "\n",
    "    r = requests.get(url)\n",
    "    file_size=int(r.headers.get('Content-Length'))\n",
    "    if os.path.exists(to_file):\n",
    "        first_byte = os.path.getsize(to_file)\n",
    "    else:\n",
    "        first_byte = 0\n",
    "    if first_byte >= file_size:  #下载完整检验\n",
    "        print(\"##### file already exists\",filename)\n",
    "        return file_size\n",
    "\n",
    "    # with open(to_file, 'wb') as fd:\n",
    "    #     for chunk in r.iter_content(chunk_size=128):\n",
    "    #         fd.write(chunk)\n",
    "    header = {\"Range\": \"bytes=%s-%s\" % (first_byte, file_size)}\n",
    "    pbar = tqdm(\n",
    "        total=file_size, initial=first_byte,\n",
    "        unit='B', unit_scale=True, desc=url.split('/')[-1])\n",
    "\n",
    "    try:\n",
    "        req = requests.get(url, headers=header, stream=True)\n",
    "        with(open(to_file, 'ab')) as f:\n",
    "            for chunk in req.iter_content(chunk_size=128):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(128)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    pbar.close()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea664eb-ea6c-431b-8b56-e5907fc11e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_hrefs=assays_fetch_newest['href']\n",
    "queue_urls=[url_org+x for x in queue_hrefs]\n",
    "\n",
    "to_path='encode/rawdata'\n",
    "# for u in queue_urls:\n",
    "#     downloader(u)\n",
    "with Pool(processes=12) as pool:  #多线程\n",
    "    pool.map(downloader,queue_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685a862-b91a-42b0-ab1f-5cc6210d61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#检验文件完整性，md5\n",
    "!md5sum encode/rawdata/* >encode/rawdata/md5sum.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154e4dd-8ea3-4ca8-8b84-8938aaf14fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "md5_df=pd.read_table('encode/rawdata/md5sum.txt',sep='\\s+',names=['md5sumx','files'])\n",
    "md5_df['accession']=md5_df['files'].str.split('/',expand=True).iloc[:,3].str.split('.',expand=True).iloc[:,0]\n",
    "\n",
    "res=pd.merge(assays_fetch_newest[['accession','md5sum','href']],md5_df,on=['accession'],how='left')\n",
    "files_imcomplete=res[res['md5sum']!=res['md5sumx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa59b1-aefd-4355-a1f5-50c3c66efee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#下载不完整的文件重新下载，重新校验\n",
    "add_hrefs=files_imcomplete['href']\n",
    "add_urls=[url_org+x for x in add_hrefs]\n",
    "\n",
    "to_path='../encode/rawdata'\n",
    "for u in add_urls:\n",
    "    downloader(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc89e65-05b3-4a0d-9807-96b53a76016a",
   "metadata": {},
   "source": [
    "#### 根据需要选择细胞系、转录因子和表观数据\n",
    "\n",
    "相关文件：  \n",
    "1. ENCODE全部星标文件，released_files_of_assays_default_samples.csv\n",
    "2. 选择细胞系、转录因子，添加统计信息（peak数量等），released_files_of_assays_default_samples_keep.csv\n",
    "3. 手动选择文件，用于模型训练，released_files_of_assays_default_samples_keep_select.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e999a-ebb2-467a-a7b1-7171fb9bc760",
   "metadata": {},
   "outputs": [],
   "source": [
    "assays_file='released_files_of_assays_default_samples.csv'\n",
    "assays=pd.read_csv(assays_file)\n",
    "print(assays.shape)\n",
    "\n",
    "file_path=assays['accession'].tolist()\n",
    "file_path=[path_rawdata+x+'.bed.gz' for x in file_path]\n",
    "assays['file_path']=file_path\n",
    "\n",
    "assays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0c65b-2cbb-44a3-86a9-79d390711482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择细胞系和表观数据\n",
    "cells = ['GM12878', 'HCT116', 'HepG2', 'IMR-90', 'K562', 'MCF-7', 'SK-N-SH'] \n",
    "chromstates=['ATAC-seq',]\n",
    "modifications=['H3K27ac','H3K27me3','H3K36me3','H3K4me1','H3K4me3','H3K9me3']\n",
    "\n",
    "#选择转录因子\n",
    "# tranfs\n",
    "tranfs_1=['ATF3', 'BHLHE40', 'CEBPB', 'CTCF', 'EGR1', 'ELF1', 'ELK1', 'FOXM1', 'GABPA', 'JUND', 'MAFK', 'MYC', 'NRF1', 'REST', 'RFX5', 'SP1', 'SRF', 'TCF7L2', 'USF1', 'USF2', 'YY1', 'ZBTB33', 'ZBTB40', 'ZNF24', 'ZNF274', 'CREB1', 'FOS', 'FOXK2', 'MAX', 'MAZ', 'MXI1', 'TCF12', 'TEAD4',]\n",
    "tranfs_2=['CHD1', 'CHD2', 'EP300', 'HDAC2', 'POLR2A', 'RAD21', 'RCOR1', 'SIN3A', 'SMC3', 'TAF1', 'TARDBP',]\n",
    "tranfs=tranfs_1+tranfs_2\n",
    "print('1类数量：',len(tranfs_1))\n",
    "print('2类数量：',len(tranfs_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61238f-739c-4007-bd8a-ce487ccbc809",
   "metadata": {},
   "outputs": [],
   "source": [
    "assays_state=assays_cell[assays_cell['assay'].isin(chromstates)]\n",
    "assays_target=assays_cell[assays_cell['target_of_assay'].isin(tranfs+modifications)]\n",
    "assays_keep=pd.concat([assays_state,assays_target]).reset_index(drop=True)\n",
    "print(assays_keep.shape)\n",
    "assays_keep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f533cf-9b49-4cc4-a850-56a948860728",
   "metadata": {},
   "source": [
    "#### peak统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2b9de-a35f-48a9-81fc-8eb11dc8ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计peak数量，初始文件数量和过滤后数量\n",
    "import pyranges as pr\n",
    "\n",
    "def stat_peak(peak_file):\n",
    "    acc=peak_file.split('/')[-1][0:11]\n",
    "    peak=pr.read_bed(peak_file)\n",
    "    lens=peak.lengths()\n",
    "    stats=lens.describe()\n",
    "    lens.acc=acc\n",
    "    stats.acc=acc\n",
    "    return lens,stats\n",
    "\n",
    "#example\n",
    "_,stats=stat_peak('ENCFF470YYO.bed.gz')\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d5ad5-e8a9-4309-885c-12d5710b48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "stats_list=[]\n",
    "for peakfile in list(assays_keep['file_path']):\n",
    "    _,stats=stat_peak(peakfile)\n",
    "    stats_list.append([math.ceil(x*100)/100 for x in list(stats)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bda13f-c89a-4179-97c2-94597530d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "assays_keep['count']=[x[0] for x in stats_list]\n",
    "assays_keep['mean']=[x[1] for x in stats_list]\n",
    "assays_keep['std']=[x[2] for x in stats_list]\n",
    "assays_keep['min']=[x[3] for x in stats_list]\n",
    "assays_keep['Q1']=[x[4] for x in stats_list]\n",
    "assays_keep['Q2']=[x[5] for x in stats_list]\n",
    "assays_keep['Q3']=[x[6] for x in stats_list]\n",
    "assays_keep['max']=[x[7] for x in stats_list]\n",
    "\n",
    "file_keep='released_files_of_assays_default_samples_keep.csv'\n",
    "assays_keep.to_csv(file_keep,index=False)\n",
    "assays_keep.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31702402-f4ff-4645-9900-d6f0a67cff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#手动挑选peak文件\n",
    "#这里注意，有的细胞系经过了稳定的基因变异处理，而非原始细胞系，相关数据不采用，如Homo sapiens SK-N-SH genetically modified (insertion) using CRISPR targeting H. sapiens GATA2\n",
    "#\n",
    "assays_select_file='released_files_of_assays_default_samples_keep_select.tsv'\n",
    "assays_select=pd.read_csv(assays_select_file,sep='\\t')\n",
    "print(assays_select.shape)\n",
    "assays_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d75ae3-d712-4d69-b80e-2cd0061d4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tranfs_sel=set(list(assays_select['target_of_assay']))\n",
    "# tranfs_sel\n",
    "tranfs_sel_1=sorted([x for x in tranfs_sel if x in tranfs_1])\n",
    "tranfs_sel_2=sorted([x for x in tranfs_sel if x in tranfs_2])\n",
    "# tranfs_sel_2\n",
    "#\n",
    "tranfs_sel=tranfs_sel_1+tranfs_sel_2\n",
    "print('1类数量：',len(tranfs_sel_1))\n",
    "print('2类数量：',len(tranfs_sel_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591034cf-7f30-4646-bb51-c96783647244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "peak_path=Path('encode/keepdata')  #data directory\n",
    "logfile=peak_path/'log.txt'\n",
    "log=open(logfile,'a')\n",
    "\n",
    "cells=['K562', 'HepG2', 'GM12878', 'MCF-7', 'SK-N-SH', 'HCT116', 'IMR-90']\n",
    "for cell in cells:\n",
    "    peak_path_cell=peak_path/cell\n",
    "    peak_path_cell.mkdir(exist_ok=True)\n",
    "    \n",
    "    cell_info=assays_select[assays_select['biosample']==cell]\n",
    "    #ATAC-seq改名\n",
    "    file_sel=cell_info[cell_info['assay']=='ATAC-seq']\n",
    "    file_from=file_sel['file_path'].tolist()[0]\n",
    "    file_to=peak_path_cell/'ATAC.bed.gz'\n",
    "    print(file_from,'-->',file_to,file=log)\n",
    "    shutil.copy(file_from,file_to)\n",
    "    \n",
    "    #histone modifications & TFs\n",
    "    for ta in modifications+tranfs_sel:\n",
    "        file_sel=cell_info[cell_info['target_of_assay']==ta]\n",
    "        try:\n",
    "            file_from=file_sel['file_path'].tolist()[0]\n",
    "            file_to=peak_path_cell/(ta+'.bed.gz')\n",
    "            print(file_from,'-->',file_to,file=log)\n",
    "            shutil.copy(file_from,file_to)\n",
    "        except:\n",
    "            print('no target:',ta,cell,file=log)\n",
    "log.close()\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0952d56a-d236-480a-9b10-7b1269791683",
   "metadata": {},
   "source": [
    "### FIMO 软件预测转录因子结合位置并与ChIP-peak比较\n",
    "\n",
    "- 预测\n",
    "\n",
    "1. 安装[FIMO](https://meme-suite.org/meme/doc/fimo.html?man_type=web)软件并准备相关运行文件\n",
    "2. 从[JASPAR](https://jaspar.elixir.no/)下载转录因子的motif文件，meme格式\n",
    "3. 运行\n",
    "\n",
    "```sh\n",
    "#!/bin/bash\n",
    "work_dir='encode/keepdata'\n",
    "\n",
    "tf_list=(ATF3 BHLHE40 CEBPB CTCF EGR1 ELF1 ELK1 FOXM1 GABPA JUND MAFK MYC NRF1 REST RFX5 SP1 SRF TCF7L2 USF1 USF2 YY1 ZBTB33 ZBTB40 ZNF24 ZNF274 CREB1 FOS FOXK2 MAX MAZ MXI1 TCF12 TEAD4)\n",
    "\n",
    "log=${work_dir}/fimo_run.log\n",
    "\n",
    "for tf in ${tf_list[@]}; do\n",
    "  echo '###'$tf running ... >> ${log};\n",
    "  meme/bin/fimo --bfile ${work_dir}/fimo/GRCh38.primary_assembly.genome.fa.bfile --thresh 1e-4 --max-stored-scores 100000 --o ${work_dir}/fimo/${tf} ${work_dir}/fimo/meme/${tf}_*.meme genome/gencode/GRCh38.primary_assembly.genome.fa &;\n",
    "  mv ${work_dir}/fimo/${tf}/fimo.gff ${work_dir}/fimo/${tf}.gff;\n",
    "done\n",
    "```\n",
    "\n",
    "- 与ChIP-peak的比较\n",
    "\n",
    "```sh\n",
    "#!/bin/bash\n",
    "work_dir='encode/keepdata'\n",
    "tf_list=(ATF3 BHLHE40 CEBPB CTCF EGR1 ELF1 ELK1 FOXM1 GABPA JUND MAFK MYC NRF1 REST RFX5 SP1 SRF TCF7L2 USF1 USF2 YY1 ZBTB33 ZBTB40 ZNF24 ZNF274 CREB1 FOS FOXK2 MAX MAZ MXI1 TCF12 TEAD4)\n",
    "ce_list=(K562 HepG2 GM12878 MCF-7 SK-N-SH HCT116 IMR-90)\n",
    "\n",
    "log=${work_dir}/fimo_compare.log\n",
    "\n",
    "for tf in ${tf_list[@]}; do\n",
    "  echo '###'${tf} `wc -l ${work_dir}/fimo/${tf}.gff`;\n",
    "  echo '###'${tf} `wc -l ${work_dir}/fimo/${tf}.gff` >> ${log};\n",
    "  for ce in ${ce_list[@]}; do\n",
    "    echo '###'${ce} >> ${log};\n",
    "    fn=${work_dir}/${ce}/${tf}.bed;\n",
    "    if [ -f ${fn} ]; then\n",
    "      mkdir ${work_dir}/${ce}/${tf};\n",
    "      bedtools intersect -a ${work_dir}/fimo/${tf}.gff -b ${fn} -v |awk -F '\\t' -v OFS='\\t' '{print $1,$4,$5,$9,$6,$7}'> ${work_dir}/${ce}/${tf}/fimo_neg.bed;\n",
    "      sleep 10;\n",
    "      bedtools intersect -a ${work_dir}/fimo/${tf}.gff -b ${fn} -u |awk -F '\\t' -v OFS='\\t' '{print $1,$4,$5,$9,$6,$7}'> ${work_dir}/${ce}/${tf}/fimo_pos.bed;\n",
    "      sleep 10;\n",
    "      echo fimo_neg.bed `wc -l ${work_dir}/encode/${ce}/${tf}/fimo_neg.bed` >> ${log};\n",
    "      echo fimo_pos.bed `wc -l ${work_dir}/encode/${ce}/${tf}/fimo_pos.bed` >> ${log};\n",
    "    fi\n",
    "  done\n",
    "done\n",
    "```\n",
    "\n",
    "*使用bedtools window*\n",
    "\n",
    "```\n",
    "#log=${work_dir}/endchip/fimo2_w.log\n",
    "#mkdir ${work_dir}/encode/${ce}/${tf}_w;\n",
    "#bedtools window -a ${work_dir}/fimo/${tf}.gff -b ${fn} -w 200 -v |awk -F '\\t' -v OFS='\\t' '{print $1,$4,$5,$9,$6,$7}'> ${work_dir}/${ce}/${tf}_w/fimo_neg.bed;\n",
    "#bedtools window -a ${work_dir}/fimo/${tf}.gff -b ${fn} -w 200 -u |awk -F '\\t' -v OFS='\\t' '{print $1,$4,$5,$9,$6,$7}'> ${work_dir}/${ce}/${tf}_w/fimo_pos.bed;\n",
    "#echo fimo_neg.bed `wc -l ${work_dir}/${ce}/${tf}_w/fimo_neg.bed` >> ${log};\n",
    "#echo fimo_pos.bed `wc -l ${work_dir}/${ce}/${tf}_w/fimo_pos.bed` >> ${log};\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4c558-3295-40da-8485-274f9fc2bc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
